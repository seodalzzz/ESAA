{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXrJtACTT8h5GtvWb9h3DC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CHAP8 텍스트 분석**"
      ],
      "metadata": {
        "id": "8m7NWD4iLPAX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NLP : 머신이 인간의 언어를 이해하고 해석하는데 중점\n",
        "* 텍스트 분석 : 비정형 텍스트에서 의미 있는 정보를 추출하는 것에 중점"
      ],
      "metadata": {
        "id": "sqg8BQkFLpWE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 텍스트 분류, 감성 분석, 텍스트 요약, 텍스트 군집화와 유사도 측정"
      ],
      "metadata": {
        "id": "VllU4RdvMGBg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**1.텍스트 분석 이해**"
      ],
      "metadata": {
        "id": "szI7G5VWMLDP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 분석 : 비정형 데이터인 텍스트를 분석하는 것\n",
        "\n",
        "텍스트를 머신러닝에 적용하기 위해서는 비정형 텍스트 데이터를 어떻게 피처 형태로 추출하고 추출된 피처에 의미있는 값을 부여하는가가 중요\n",
        "\n",
        "피처 벡터화(피처 추출) > BOW, Word2Vec"
      ],
      "metadata": {
        "id": "B3D1h4aOMOPM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) 텍스트 분석 수행 프로세스\n",
        "\n",
        "1. 텍스트 사전 준비작업 (텍스트 전처리) : 텍스트를 피처로 만들기 전에 미리 클렌징, 대/소문자 변경, 특수문자 삭제 등의 클렌징 작업, 단어 등의 토큰화 작업, 의미없는 단어 제거 작업, 어근 추출 등의 텍스트 정규화 작업으 수행하는 것을 통칭\n",
        "\n",
        "2. 피처 벡터화/추출 : 사전 준비 작업으로 가공된 텍스트에서 피처를 추출하고 여기에 벡터 값을 할당 -> BOW\n",
        "\n",
        "3. ML모델 쉽 및 학습/예측/평가 : 피처 벡터화된 데이터 세트에 ML모델을 적용해 학습/예측 및 평가를 수행"
      ],
      "metadata": {
        "id": "8-qt24zJMh0d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2) 파이썬 기반의 NLP, 텍스트 분석 패키지\n",
        "* NTLK:방대한 데이터 세트와 서브 모듈을 가지고 있으며 NLP의 거의 모든 영역을 커버하고 있음. 수행 속도 측면에서 아쉬운 부분이 있어 샐제 대량의 데이터 기반에서는 제대로 활용 X\n",
        "\n",
        "* Gensim : 토픽 모델링 분야에서 두각을 나타내는 패키지\n",
        "\n",
        "* SpaCy : 최근 가장 주목받는 NLP패키지"
      ],
      "metadata": {
        "id": "LRqLu86fNAbu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**2.텍스트 사전 준비 작업(텍스트 전처리) - 텍스트 정규화**"
      ],
      "metadata": {
        "id": "a-E1xo93Na1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "텍스트 자체를 바로 피처로 만들수는 없음 > 정규화 작업 필요\n",
        "\n",
        "정규화 : 클렌징, 정제, 토큰화, 어근화 등의 다양한 텍스트 사전 작업"
      ],
      "metadata": {
        "id": "7p_GkIsFNgfT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) 클렌징\n",
        "\n",
        "텍스트에서 분석에 오히려 방해가 되는 불필요한 문자, 기호 등으 사전에 제거하는 작업 \n"
      ],
      "metadata": {
        "id": "-Jsyy6NPNuk9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2) 텍스트 토큰화\n",
        "\n"
      ],
      "metadata": {
        "id": "IMSS7nX2N3Ad"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "####(1) 문장 토큰화\n",
        "\n",
        "문장의 마침표, 개행문자(\\n) 등 문장의 마지막을 뜻하는 기호에 따라 분리\n",
        "\n",
        "send_tokenize() 는 각각의 문장으로 구성된 리스트를 반환"
      ],
      "metadata": {
        "id": "nUl4ZjkoN8Ot"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c4Zu7QQoJw3p",
        "outputId": "bde2615a-998d-467d-bb0d-4c8597f0b042"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room. \\\n",
        "               You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "\n",
        "sentences = sent_tokenize(text=text_sample) ## sent_tokenize 는 각각의 문장을 담은 list 형태로 반환\n",
        "print(type(sentences),len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#####(2) 단어 토큰화\n",
        "\n",
        "문장을 단어로 토큰화\n",
        "\n",
        "공백, 콤마, 마침표, 개행문자 등으로 단어를 분리"
      ],
      "metadata": {
        "id": "IqXxpka5OcML"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "\n",
        "sentence = \"The Matrix is everywhere its all around us, here even in this room.\"\n",
        "\n",
        "words = word_tokenize(sentence)\n",
        "print(type(words), len(words))\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r1quS1FKOqUW",
        "outputId": "073542db-9c04-498e-faee-f703e318485c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize, sent_tokenize\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \n",
        "    ## 문장 토큰화\n",
        "    sentences = sent_tokenize(text)\n",
        "    ## 단어 토큰화\n",
        "    word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "\n",
        "    return word_tokens\n",
        "\n",
        "## 문서 입력 및 문장/단어 토큰화 수행\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "\n",
        "## 출력\n",
        "print(type(word_tokens),len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y5RYFrmOOxdO",
        "outputId": "71025d9a-4ad9-49fd-8047-d0db0638d710"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장을 단어별로 하나씩 토큰화 할 경우 문맥적인 의미는 무시됨 > n-gram\n",
        "\n",
        "n-gram : 연속된 n개의 단어를 하나의 토큰화 단위로 분리"
      ],
      "metadata": {
        "id": "8RtpBvkaOsXZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3) 스톱 워드 제거"
      ],
      "metadata": {
        "id": "iMcnm-stO9Rw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "스톱워드 : 분석에 큰 의미가 없는 단어 (is, the, a 등)\n",
        "\n",
        "스톱워드는 빈번하게 등장하는 경우가 많기 때문에 스톱워드를 제거하지 않을 경우 오히려 중요한 단어로 인식될 가능성이 있음"
      ],
      "metadata": {
        "id": "WBjycaM6PBH_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTecS21aPKRU",
        "outputId": "995cd485-5d02-4851-c43c-c0f65aa0367a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('영어 stop words 갯수:',len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:20])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2_fSQPtPViS",
        "outputId": "12c88044-7d4a-41db-9376-e8fcae8dc7a4"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 갯수: 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "\n",
        "for sentence in word_tokens: # 3개의 각 문장을 확인\n",
        "    filtered_words = []\n",
        "    \n",
        "    for word in sentence:\n",
        "        \n",
        "        # 대문자 -> 소문자\n",
        "        word = word.lower()\n",
        "        \n",
        "        # 불용어가 아니라면, 리스트에 추가\n",
        "        if word not in stopwords:\n",
        "            filtered_words.append(word)\n",
        "    \n",
        "    all_tokens.append(filtered_words)\n",
        "    \n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JviAr3txPaTs",
        "outputId": "57e1865f-07f4-4cf6-f00b-338afce1ad6c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4) Stemming 과 Lemmatization\n",
        "\n",
        "단어의 원형 찾기\n",
        "\n",
        "Stemming : 원형 단어로 변환 시 일반적인 방법을 적용하거나 더 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향이 있음\n",
        "\n",
        "Lemmatization : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근을 찾아줌\n",
        "\n",
        "NLTK의 Stemmer : Porter, Lancaster, Snowball Stemmer\n",
        "\n",
        "NLTK의 Lemmatization : WordNetLemmatizer"
      ],
      "metadata": {
        "id": "8Hs1Mp66PcDQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "\n",
        "print(stemmer.stem('working'),stemmer.stem('works'),stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing'),stemmer.stem('amuses'),stemmer.stem('amused'))\n",
        "print(stemmer.stem('happier'),stemmer.stem('happiest'))\n",
        "print(stemmer.stem('fancier'),stemmer.stem('fanciest'))"
      ],
      "metadata": {
        "id": "OSSCUGQWQEY-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "> amuse의 경우 amus에 ing, s, ed가 붙으므로 amus를 원형으로 인식함"
      ],
      "metadata": {
        "id": "yF6V-C1AQLu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VcNsrDX-QRv4",
        "outputId": "eb3be57b-7425-4b0d-be99-ee7da7fbfccc"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemma = WordNetLemmatizer()\n",
        "\n",
        "print(lemma.lemmatize('amusing', 'v'),lemma.lemmatize('amuses', 'v'),lemma.lemmatize('amused', 'v'))\n",
        "print(lemma.lemmatize('happier', 'a'),lemma.lemmatize('happiest', 'a'))\n",
        "print(lemma.lemmatize('fancier', 'a'),lemma.lemmatize('fanciest', 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6xVEK_WGQTUt",
        "outputId": "65146bc4-fa0c-4ad3-b20f-7554198da03b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse amuse\n",
            "happy happy\n",
            "fancy fancy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**3. Bag of Words - BOW**"
      ],
      "metadata": {
        "id": "ObrFzfScQWzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "BOW : 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델\n",
        "\n",
        "\n",
        "쉽고 빠른 구축\n",
        "\n",
        "단점\n",
        "  * 문맥 의미 반영 부족 : 단어의 순서를 고려하지 않음\n",
        "  * 희소 행렬 문제 : BOW로 피처 벡터화를 수행하면 희소 행렬 형태의 데이터 세트가 만들어지기 쉬움\n"
      ],
      "metadata": {
        "id": "y-3xJgcTQdKM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###1) BOW 피처 벡터화\n",
        "\n",
        "ex) 각 문서의 텍스트를 단어로 추출해 피처로 할당하고, 각 단어의 발생 빈도와 같은 값을 피처에 값으로 부여\n",
        "\n",
        "BOW의 피처 벡터화 : 모든 문서에서 모든 단어를 칼럼 형태로 나여하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것\n",
        "  * Count 기반의 벡터화 : 단어의 빈도수를 부여\n",
        "  * TF-IDF 기반의 벡터화 : 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서는 페널티를 줌"
      ],
      "metadata": {
        "id": "pP3TpukOQ2uV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2) 사이킷런의 Count 및 TF-IDF 벡터화 구현 : CountVectorizer, TidfVectorizer\n",
        "\n",
        "파라미터 : max_df, min_df, max_features, stop_words, n_gram_range, analyzer, token_pattern, tokenizer"
      ],
      "metadata": {
        "id": "xnSnJb-XTEcv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###3) BOW 벡터화를 위한 희소 행렬\n",
        "\n",
        "희소 행렬은 너무 많은 불필요한 0 값이 메모리 공간에 할당됨\n",
        "-> 물리적으로 적은 메모리 공간을 차지하도록 변환 필요"
      ],
      "metadata": {
        "id": "9gunNx0qTi-j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###4) 희소 행렬- COO 형식\n",
        "\n",
        "0이 아닌 데이터만 별도의 배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장하는 방식\n",
        "\n",
        "희소 행렬 변환을 위해 사이파이 이용"
      ],
      "metadata": {
        "id": "LzV1O094Tz0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "dense = np.array([[3, 0, 1], [0, 2, 0]])"
      ],
      "metadata": {
        "id": "GIV3PEorQTPU"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "## 0 이 아닌 데이터 추출\n",
        "data = np.array([3, 1, 2])\n",
        "\n",
        "## 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0, 0, 1])\n",
        "col_pos = np.array([0, 2, 1])\n",
        "\n",
        "## sparse 패키지의 coo_matrix를 이용하여 COO 형식으로 희소 행렬 생성\n",
        "sparse_coo = sparse.coo_matrix((data, (row_pos, col_pos))) "
      ],
      "metadata": {
        "id": "UYib-VO1UJ1b"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "toarray()를 이용해 다시 밀집 형태의 행렬로 출력"
      ],
      "metadata": {
        "id": "ET4buKj_UOco"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sparse_coo.toarray() "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OZl-aJmxUSm5",
        "outputId": "9fed5cf7-ab95-4f32-fe7f-78ec70a24a97"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[3, 0, 1],\n",
              "       [0, 2, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###5) 희소 해열 - CSR 형식\n",
        "\n",
        "COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식\n",
        "\n",
        "행 위치 배열이 0부터 순차적으로 증가하는 값으로 이루어 졌다는 특성을 고려하면 행 위치 배열의 고유한 값의 시작 위치만 표기하는 방법으로 반복 제거 가능(위치의 위치를 표기)"
      ],
      "metadata": {
        "id": "s7Qzxhk1UVKI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import sparse\n",
        "\n",
        "dense2 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])"
      ],
      "metadata": {
        "id": "8Pha7X2ZUvO2"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 0 이 아닌 데이터 추출\n",
        "data2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
        "\n",
        "## 행 위치와 열 위치를 각각 array로 생성\n",
        "row_pos = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
        "col_pos = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
        "\n",
        "## COO 형식으로 변환 \n",
        "sparse_coo = sparse.coo_matrix((data2, (row_pos,col_pos)))\n",
        "\n",
        "## 행 위치 배열의 고유한 값들의 시작 위치 인덱스를 배열로 생성\n",
        "row_pos_ind = np.array([0, 2, 7, 9, 10, 12, 13])\n",
        "\n",
        "## CSR 형식으로 변환 \n",
        "sparse_csr = sparse.csr_matrix((data2, col_pos, row_pos_ind))\n",
        "\n",
        "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_coo.toarray())\n",
        "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인')\n",
        "print(sparse_csr.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTtuy9sMUwd6",
        "outputId": "ff25c099-b5ed-4fa0-b395-8addeaebb3c6"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n",
            "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력 확인\n",
            "[[0 0 1 0 0 5]\n",
            " [1 4 0 3 2 5]\n",
            " [0 6 0 3 0 0]\n",
            " [2 0 0 0 0 0]\n",
            " [0 0 0 7 0 8]\n",
            " [1 0 0 0 0 0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 실제 사용 시에는 다음과 같이 밀집 행렬을 생성 파라미터로 입력하면 COO나 CSR 희소 행렬로 생성함"
      ],
      "metadata": {
        "id": "k41Pf78XU6yX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dense3 = np.array([[0,0,1,0,0,5],\n",
        "                  [1,4,0,3,2,5],\n",
        "                  [0,6,0,3,0,0],\n",
        "                  [2,0,0,0,0,0],\n",
        "                  [0,0,0,7,0,8],\n",
        "                  [1,0,0,0,0,0]])\n",
        "\n",
        "coo = sparse.coo_matrix(dense3)\n",
        "csr = sparse.csr_matrix(dense3)"
      ],
      "metadata": {
        "id": "v6RouZVJU4xE"
      },
      "execution_count": 14,
      "outputs": []
    }
  ]
}