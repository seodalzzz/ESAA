{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOaCfczzGDZKPYZMy/lED2K"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "https://dacon.io/competitions/official/235670/codeshare/1841?page=2&dtype=recent"
      ],
      "metadata": {
        "id": "mCQSsXifFjBP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1. Facebook 의 fasttext를 이용한 text classification\n"
      ],
      "metadata": {
        "id": "Yw56Vw2UaFN4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "id": "YvBnGF6HZ9aJ",
        "outputId": "7cd3af7f-4daf-4825-e848-0cdd9ac07c0d"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-37f9a7044267>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfastText\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# # Word Representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# # Skipgram model :\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# skip_model = fasttext.train_unsupervised('./author.txt', model='skipgram')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fastText'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import fastText\n",
        "\n",
        "# # Word Representation\n",
        "# # Skipgram model :\n",
        "# skip_model = fasttext.train_unsupervised('./author.txt', model='skipgram')\n",
        "# skip_model.save_model(\"skip_model.bin\")\n",
        "# print(skip_model.words)\n",
        "# # or, cbow model :\n",
        "# cbow_model = fasttext.train_unsupervised('./author.txt', model='cbow')\n",
        "# cbow_model.save_model(\"cbow_model.bin\")\n",
        "# print(cbow_model.words)\n",
        "\n",
        "#  디폴트로 돌리면 0.56\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "train =pd.read_csv(\"train.csv\")\n",
        "\n",
        "file=open('fasttexttrain.txt','w+')\n",
        "for i in train.index:\n",
        "  line='__label__'+str(train['author'][i])+''+train['text'][i]\n",
        "  file.write(line+'\\n')\n",
        "\n",
        "text_clf_model = fasttext.train_supervised('fasttexttrain.txt', epoch=30, minCount=2, maxn=10, verbose=0)\n",
        "print(text_clf_model.words)\n",
        "print(text_clf_model.labels)\n",
        "\n",
        "reuslt = text_clf_model.predict(\"He was almost choking. There was so much, so much he wanted to say, but strange exclamations were all that came from his lips. The Pole gazed fixedly at him, at the bundle of notes in his hand; looked at odin, and was in evident perplexity.\", k=5)\n",
        "print(reuslt)\n",
        "\n",
        "test = pd.read_csv('../input/fasttext/test_x.csv')\n",
        "submission = pd.read_csv('../input/fasttext/sample_submission.csv', index_col=False)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in test.index:\n",
        "    lable, proba = text_clf_model.predict(test['text'][i], k=5)\n",
        "    for la, pr in zip(lable, proba):\n",
        "        if '__label__0' == la:\n",
        "            submission.loc[i, '0'] = pr\n",
        "        elif '__label__1' == la:\n",
        "            submission.loc[i, '1'] = pr\n",
        "        elif '__label__2' == la:\n",
        "            submission.loc[i, '2'] = pr\n",
        "        elif '__label__3' == la:\n",
        "            submission.loc[i, '3'] = pr\n",
        "        elif '__label__4' == la:\n",
        "            submission.loc[i, '4'] = pr\n",
        "    # submission.loc[i, '0'] = proba[lable.loc('__label__0')]\n",
        "    # submission.loc[i, '1'] = proba[4]\n",
        "    # submission.loc[i, '2'] = proba[2]\n",
        "    # submission.loc[i, '3'] = proba[0]\n",
        "    # submission.loc[i, '4'] = proba[3]\n",
        "\n",
        "submission.to_csv('result5_fasttext.csv', index=False)\n",
        "print('end')"
      ],
      "metadata": {
        "id": "71KfwUOzA4_7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. NLP Processing\n",
        "\n",
        "(1) 데이터를 불러온 후 각 신문기사들을 눈으로 확인하며 특수문자, 불용어 그리고 문장 구조에 대한 감을 잡습니다.\n",
        "\n",
        "(2) 문제의 목적과 분석자의 재량에 따라 불용어를 설정하고 리스트에 저장합니다. 이번 대회에서는 특수 문자와 조사만 제거해도 어느 정도 높은 정확도를 얻을 수 있습니다.\n",
        "\n",
        "(3) 불용어 이외의 특수 문자들을 제거합니다.\n",
        "\n",
        "(4) 형태소 분석을 통해 문장을 형태소 단위의 토큰으로 분리합니다. \n",
        "\n",
        "(5) 형태소 단위의 토큰들을 기반으로 리스트에 저장된 불용어를 제거합니다."
      ],
      "metadata": {
        "id": "aqCyMXKqBNOK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 형태소 분석\n",
        "* 단어나 문장의 언어적 속성을 파악하는 것\n",
        "* Konlpy 패키지\n",
        "* 형태소 분석을 하는 이유는 주로 형태소 단위로 의미있는 단어를 가져가고 싶거나 품사 태깅을 통해 형용사나 명사를 추출하고싶을 때 이용\n",
        "* 무장을 띄어쓰기 단위로 분류하여 vectoriation을 하게 디면 같은 의미의 토큰 세개가 서로 다른 vector를 가지게 됨. 하지만 형태소 분석을 통해 토큰을 추출하면 같은 단어는 동일한 vector를 가지게 됨"
      ],
      "metadata": {
        "id": "ftWC_xUpBrbJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) Kkma()"
      ],
      "metadata": {
        "id": "uUcUBf6gCPie"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "jX3s9q4SBL9-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Okt()"
      ],
      "metadata": {
        "id": "ZqGWgtGZCRBx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "Okt = Okt()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(Okt.pos(sentence))"
      ],
      "metadata": {
        "id": "cTApsyILCUcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Mecab()"
      ],
      "metadata": {
        "id": "wWvYULaOCggL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Mecab \n",
        "Mecab  = Mecab ()\n",
        "\n",
        "sentence = '데이콘에서 다양한 컴피티션을 즐기면서 실력있는 데이터 분석가로 성장하세요!!.'\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(Mecab .morphs(sentence))\n",
        "print(\" \")\n",
        "print(\"문장에서 명사 추출\")\n",
        "print(\"----------------------\")\n",
        "print(Mecab .nouns(sentence))\n",
        "print(\" \")\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(Mecab .pos(sentence))"
      ],
      "metadata": {
        "id": "1VqltVDmCjhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. 표제어 추출(Lemmatization)"
      ],
      "metadata": {
        "id": "V9KPoIdIClKB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Kkma\n",
        "kkma = Kkma()\n",
        "\n",
        "sentence = '성장했었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "bW2duEcrCuyT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence = '성장하였었다.'\n",
        "\n",
        "print(\"품사 태킹(PoS)\")\n",
        "print(\"----------------------\")\n",
        "print(kkma.pos(sentence))"
      ],
      "metadata": {
        "id": "-W4qs7m5CwIP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. 불용어 제거"
      ],
      "metadata": {
        "id": "8pQJourbC0YI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 불용어 : 문장에서 큰 의미가 없다고 생각되는 단어"
      ],
      "metadata": {
        "id": "rE11tI7eC3cS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "tokenizer = Okt()\n",
        "def text_preprocessing(text,tokenizer):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는']\n",
        "    \n",
        "    txt = re.sub('[^가-힣a-z]', ' ', text)\n",
        "    token = tokenizer.morphs(txt)\n",
        "    token = [t for t in token if t not in stopwords]\n",
        "        \n",
        "    return token\n",
        "\n",
        "ex_text = \"이번에 새롭게 개봉한 영화의 배우들은 모두 훌륭한 연기력과 아름다운 목소리를 갖고 있어!!\"\n",
        "example_pre= text_preprocessing(ex_text,tokenizer)"
      ],
      "metadata": {
        "id": "oa4Y6BVDDJ0w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 영어 소문자와 한글을 제외한 모든 문자를 제거\n",
        "* Okt를 이용해 형태소 분석\n",
        "* 형태소 분석기를 거쳐 나온 결과들 중 stopwords 리스트에 포함되지 않는 토큰만 token이라는 리스트에 반환"
      ],
      "metadata": {
        "id": "LZaBaAA1DMXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(example_pre)"
      ],
      "metadata": {
        "id": "PeKed80-DWkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. 대회 적용\n"
      ],
      "metadata": {
        "id": "tAZJgZc3DXuO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def text_preprocessing(text_list):\n",
        "    \n",
        "    stopwords = ['을', '를', '이', '가', '은', '는', 'null'] #불용어 설정\n",
        "    tokenizer = Okt() #형태소 분석기 \n",
        "    token_list = []\n",
        "    \n",
        "    for text in text_list:\n",
        "        txt = re.sub('[^가-힣a-z]', ' ', text) #한글과 영어 소문자만 남기고 다른 글자 모두 제거\n",
        "        token = tokenizer.morphs(txt) #형태소 분석\n",
        "        token = [t for t in token if t not in stopwords or type(t) != float] #형태소 분석 결과 중 stopwords에 해당하지 않는 것만 추출\n",
        "        token_list.append(token)\n",
        "        \n",
        "    return token_list, tokenizer\n",
        "\n",
        "#형태소 분석기를 따로 저장한 이유는 후에 test 데이터 전처리를 진행할 때 이용해야 되기 때문입니다. \n",
        "train['new_article'], okt = text_preprocessing(train['content']) "
      ],
      "metadata": {
        "id": "gUzDoOiqDbNa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Vectorization"
      ],
      "metadata": {
        "id": "f2djWjRVDysF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* NLP를 컴퓨터가 이해할 수 있게 수치로 바꾸는 것"
      ],
      "metadata": {
        "id": "aXbbpLn_D3_A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt\n",
        "import re\n",
        "\n",
        "Okt = Okt()\n",
        "\n",
        "sentences = ['자연어 처리는 정말 정말 즐거워.', '즐거운 자연어 처리 다같이 해보자.']\n",
        "tokens = []\n",
        "\n",
        "for sentence in sentences:\n",
        "    sentence = re.sub('[^가-힣a-z]', ' ', sentence) #간단한 전처리\n",
        "    token = (Okt.morphs(sentence)) #형태소 분석기를 이용햔 토큰 나누기\n",
        "    tokens.append(' '.join(token))\n",
        "\n",
        "print(\"형태소 단위로 문장 분리\")\n",
        "print(\"----------------------\")\n",
        "print(tokens)"
      ],
      "metadata": {
        "id": "FoEyvp3HEEpD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1) One Hot Encoding\n",
        "\n",
        "* 해당 단어가 존재하면 1, 그렇지 않으면 모두 0으로 표시되는 기법"
      ],
      "metadata": {
        "id": "MNxD81VkEFlm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "t = Tokenizer()\n",
        "t.fit_on_texts(tokens)\n",
        "print(\"각 토큰에게 고유의 정수 부여\")\n",
        "print(\"----------------------\")\n",
        "print(t.word_index) \n",
        "print(\" \")\n",
        "\n",
        "s1=t.texts_to_sequences(tokens)[0] \n",
        "print(\"부여된 정수로 표시된 문장1\")\n",
        "print(\"----------------------\")\n",
        "print(s1)\n",
        "print(\" \")\n",
        "\n",
        "s2=t.texts_to_sequences(tokens)[1]\n",
        "print(\"부여된 정수로 표시된 문장2\")\n",
        "print(\"----------------------\")\n",
        "print(s2)\n",
        "print(\" \")\n",
        "\n",
        "s1_one_hot = to_categorical(s1)\n",
        "print(\"문장1의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s1_one_hot)\n",
        "print(\" \")\n",
        "\n",
        "s2_one_hot = to_categorical(s2)\n",
        "print(\"문장2의 one-hot-encoding\")\n",
        "print(\"----------------------\")\n",
        "print(s2_one_hot)"
      ],
      "metadata": {
        "id": "vab03KIiEVXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2) Count vectorization\n",
        "\n",
        "* vocabulary를 활용하여 각 문장이 갖고 있는 토큰의 count를 기반으로 문장을 vectorization"
      ],
      "metadata": {
        "id": "MJ2El4htEWzH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "vectors = vectorizer.fit_transform(tokens) #여러 개의 문장을 넣어줘야 작동합니다!!\n",
        "\n",
        "print(vectorizer.get_feature_names())\n",
        "print(vectors.toarray())"
      ],
      "metadata": {
        "id": "fRRGhpxKEeMY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 즐거운과 즐거워는 같은 의미를 갖는 토큰이지만 okt는 이를 구분해주지 못함"
      ],
      "metadata": {
        "id": "4Hwwd9gYEjwb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3) Tfldf\n",
        "\n",
        "* 단어가 몇번 등장했는지에 대한 정보\n",
        "* 어떤 단어가 언급된 문서의 수가 적다면 그 단어는 문서를 분류하는데 있어서 중요한 단어"
      ],
      "metadata": {
        "id": "coF9HBYJEoxj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "tfidf = TfidfVectorizer(min_df=0)\n",
        "tfidf_vectorizer = tfidf.fit_transform(tokens) \n",
        "\n",
        "#tf-idf dictionary    \n",
        "tfidf_dict = tfidf.get_feature_names()\n",
        "print(tfidf_dict)\n",
        "print(tfidf_vectorizer.toarray())"
      ],
      "metadata": {
        "id": "lSfp3VGSExP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4. Padding\n",
        "\n",
        "* 문장의 길이를 동일하게 맞춰주어야 함 > 문장 길이를 맞추기 위해 부족한 길이만큼 0을 채워넣는 작업"
      ],
      "metadata": {
        "id": "ffFrokUeE3gG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. 대회 적용"
      ],
      "metadata": {
        "id": "tPimZ-1hFarV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "def text2sequence(train_text, max_len=100):\n",
        "    \n",
        "    tokenizer = Tokenizer() #keras의 vectorizing 함수 호출\n",
        "    tokenizer.fit_on_texts(train_text) #train 문장에 fit\n",
        "    train_X_seq = tokenizer.texts_to_sequences(train_text) #각 토큰들에 정수 부여\n",
        "    vocab_size = len(tokenizer.word_index) + 1 #모델에 알려줄 vocabulary의 크기 계산\n",
        "    print('vocab_size : ', vocab_size)\n",
        "    X_train = pad_sequences(train_X_seq, maxlen = max_len) #설정한 문장의 최대 길이만큼 padding\n",
        "    \n",
        "    return X_train, vocab_size, tokenizer\n",
        "\n",
        "train_X, vocab_size, vectorizer = text2sequence(train['text'], max_len = 100)"
      ],
      "metadata": {
        "id": "iiECQs4OFdhn"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}